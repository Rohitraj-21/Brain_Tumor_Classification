# -*- coding: utf-8 -*-
"""RR1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P_edRYxzlfRnHsXyo0xXo4nCCm34fUXy
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("masoudnickparvar/brain-tumor-mri-dataset")

print("Path to dataset files:", path)

import os

# Path to dataset
dataset_path = path

# Walk through the directory structure
for root, dirs, _ in os.walk(dataset_path):
    print(f"\n📁 Directory: {root}")
    for d in dirs:
        print(f"   └── 📂 Sub-folder: {d}")

import os
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Set paths
train_dir = f'{path}/Training'
test_dir = f'{path}/Testing'

# Define transforms for training and testing
transform = transforms.Compose([
    transforms.Resize((224, 224)),         # Resize images
    transforms.ToTensor(),                 # Convert to Tensor
    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale images
])

# Load dataset using ImageFolder
train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)
test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Print class names and dataset size
print("Classes:", train_dataset.classes)
print(f"Number of training samples: {len(train_dataset)}")
print(f"Number of testing samples: {len(test_dataset)}")

import matplotlib.pyplot as plt
import torchvision

# Get one batch of training images
images, labels = next(iter(train_loader))

# Make a grid of first 8 images
img_grid = torchvision.utils.make_grid(images[:8], nrow=4)
plt.figure(figsize=(10, 4))
plt.imshow(img_grid.permute(1, 2, 0))  # Convert from CxHxW to HxWxC
plt.title("Sample Training Images")
plt.axis('off')
plt.show()

import torch
import torch.nn as nn
from torchvision import models

# Use GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load pretrained ResNet50 model
model = models.resnet50(pretrained=True)

# Unfreeze all layers (make all parameters trainable)
for param in model.parameters():
    param.requires_grad = True

# Replace the classifier (fc layer) with a custom one
model.fc = nn.Sequential(
    nn.Linear(model.fc.in_features, 512),
    nn.BatchNorm1d(512),
    nn.ReLU(),
    nn.Dropout(0.5),

    nn.Linear(512, 256),
    nn.BatchNorm1d(256),
    nn.ReLU(),
    nn.Dropout(0.4),

    nn.Linear(256, 4)  # 4 classes output
)


# Move model to device
model = model.to(device)

import torch.optim as optim

# Loss function
criterion = nn.CrossEntropyLoss()

# Optimizer (only for trainable parameters)
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)

def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs=10):
    best_val_acc = 0
    patience = 3       # number of epochs to wait after last improvement
    counter = 0

    train_losses = []
    val_losses = []
    train_accs = []
    val_accs = []

    for epoch in range(epochs):
        running_loss = 0.0
        correct = 0
        total = 0

        # Training phase
        model.train()
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # Accuracy
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            running_loss += loss.item()

        train_acc = 100 * correct / total
        avg_train_loss = running_loss / len(train_loader)

        # Validation phase
        model.eval()
        val_correct = 0
        val_total = 0
        val_running_loss = 0.0
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_running_loss += loss.item()

                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()

        val_acc = 100 * val_correct / val_total
        avg_val_loss = val_running_loss / len(val_loader)

        # Append metrics for plotting
        train_losses.append(avg_train_loss)
        val_losses.append(avg_val_loss)
        train_accs.append(train_acc)
        val_accs.append(val_acc)

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            counter = 0
            # Save the best model here
            torch.save(model.state_dict(), "best_model.pth")
            print(f"Model saved at epoch {epoch+1} with val_acc: {best_val_acc:.2f}%")
        else:
            counter += 1

        if best_val_acc >= 100:  # your accuracy threshold
            print(f"Early stopping: val_acc reached {best_val_acc:.2f}%")
            break

        if counter >= patience:
            print(f"Early stopping: no improvement in {patience} epochs")
            print(f"validation accuracy : {best_val_acc}")
            break

        print(f"Epoch [{epoch+1}/{epochs}] ➜ Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%")

    return train_losses, val_losses, train_accs, val_accs

train_losses, val_losses, train_accs, val_accs = train_model(
    model, train_loader, test_loader, criterion, optimizer, device, epochs=10)

import matplotlib.pyplot as plt

def plot_metrics(train_losses, val_losses, train_accs, val_accs):
    epochs = range(1, len(train_losses) + 1)

    plt.figure(figsize=(14,5))

    # Loss plot
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_losses, label='Training Loss')
    plt.plot(epochs, val_losses, label='Validation Loss')
    plt.title('Loss over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Accuracy plot
    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_accs, label='Training Accuracy')
    plt.plot(epochs, val_accs, label='Validation Accuracy')
    plt.title('Accuracy over epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()

    plt.show()


plot_metrics(train_losses, val_losses, train_accs, val_accs)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import numpy as np

def plot_confusion_matrix(model, data_loader, device, class_names):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in data_loader:
            images = images.to(device)
            outputs = model(images)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())

    cm = confusion_matrix(all_labels, all_preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap=plt.cm.Blues)
    plt.title("Confusion Matrix")
    plt.show()

class_names = ['glioma', 'meningioma', 'notumor', 'pituitary']
plot_confusion_matrix(model, test_loader, device, class_names)